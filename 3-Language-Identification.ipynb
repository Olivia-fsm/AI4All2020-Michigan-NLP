{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Save2Drive](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/save2drive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "### Introduction\n",
    "0. Brief introductory reading\n",
    "\n",
    "### Implement Simple Model\n",
    "1. Implementation: Preprocessing\n",
    "2. Implementation: Training the simple model\n",
    "3. Brief reading about evaluation\n",
    "4. Implementation: Making predictions\n",
    "5. Evaluating the simple model\n",
    "6. Analysis\n",
    "\n",
    "### Implement Bigram Model\n",
    "6. Implementation: Preprocessing\n",
    "7. Implementation: Training the bigram model\n",
    "8. Implementation: Making predictions\n",
    "9. Evaluating the bigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introductory Reading (~5 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Language Identification](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/3-language-identification.png)\n",
    "![Language Identification](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/3-language-identification-2.png)\n",
    "![Language Identification](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/3-language-identification-3.png)\n",
    "![Language Identification](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/3-language-identification-4.png)\n",
    "![Language Identification](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/3-language-identification-5.png)\n",
    "![Language Identification](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/3-language-identification-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Run the cell below to get setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - run\n",
    "import sys, os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  !rm -r AI4All2020-Michigan-NLP\n",
    "  !git clone https://github.com/alahnala/AI4All2020-Michigan-NLP.git\n",
    "  !cp -r AI4All2020-Michigan-NLP/utils/ .\n",
    "  !cp -r AI4All2020-Michigan-NLP/Data/ .\n",
    "  !cp -r AI4All2020-Michigan-NLP/slides/ .\n",
    "  !cp -r AI4All2020-Michigan-NLP/Experiment-Report-Templates/ .\n",
    "  !echo \"=== Files Copied ===\"\n",
    "\n",
    "from utils.lang_detect_helpers import *\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.3 Make a copy of the experiment documentation template to document your experiments\n",
    "\n",
    "## [Link to Language-Identification-Template Google Slides](https://docs.google.com/presentation/d/1yDgUwbu46-iHJJdiDHDCwNDUamwKqOuh1JNoo0AqAic/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Write a preprocess function. Refer to 1-Intro-to-NLP.ipynb for ideas!\n",
    "\n",
    "Try out different test strings to see how your function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    '''\n",
    "    (Template) Implement preprocessing for language detection. Input is a string, and output should be a list of strings.\n",
    "    '''\n",
    "    \n",
    "    # should be a list of strings\n",
    "    preprocessed_sentence = [sentence]\n",
    "    \n",
    "    return preprocessed_sentence \n",
    "\n",
    "test_string = 'Hello!! Making a system that can tell me the language of a string of text will unlock many possibilities! :D'\n",
    "preprocess(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Run the cells to preprocess each language's text\n",
    "\n",
    "We imported a data structure called `train`, which is a dictionary. Each key in the dictionary is a language. The value for each language is a list of sentence strings that are in that language.\n",
    "\n",
    "The second cell below will go through each language and get its `text_list` from the `train` dictionary, then go through each `sentence` in `text_list` and use your preprocessing function on it to get a list of tokens. The `preprocessed_list_of_tokens` will be saved in a new dictionary called `training_data`, which is very similar to `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Languages we are training on:\", languages)\n",
    "print(\"First 3 text strings in English's training set:\", train['English'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = {}\n",
    "\n",
    "for language in languages:\n",
    "    \n",
    "    training_data[language] = []\n",
    "    text_list = train[language]\n",
    "    \n",
    "    \n",
    "    for sentence in text_list:\n",
    "        preprocessed_list_of_tokens = preprocess(sentence)\n",
    "        training_data[language].append(preprocessed_list_of_tokens)\n",
    "        \n",
    "print(\"First 3 text strings as preprocessed list of tokens in English's training set:\", training_data['English'][:3])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Documentation - Take a brief second to write down the preprocessing steps you are trying (you can use `Preprocessing Experiment 1` slide in Language-Identification-Template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implement the `train` function \n",
    "\n",
    "- Implement the `train` function for our simplified model we introduced by filling in the TODOs. \n",
    "- When computing the probabilities, use **Add One Smoothing** which is described below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Language Identification](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/3-language-identification-7.png)\n",
    "![Language Identification](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/3-language-identification-8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_dictionaries = {l:{} for l in languages}\n",
    "\n",
    "\n",
    "def count_tokens(preprocessed_list_of_tokens):\n",
    "    unigram_counter = defaultdict(lambda:0)\n",
    "            \n",
    "    for '''TODO''' in '''TODO''':\n",
    "        for '''TODO''' in '''TODO''':\n",
    "            unigram_counter['''TODO'''] += 1\n",
    "    return unigram_counter\n",
    "\n",
    "\n",
    "def compute_probabilities(unigram_counts, vocabulary):\n",
    "    \n",
    "    unigram_probabilities = defaultdict(lambda:0)\n",
    "    \n",
    "    total_unigrams = sum(unigram_counts.values())\n",
    "    \n",
    "    \n",
    "    for unigram in vocabulary:\n",
    "        \n",
    "        variable_1 = '''TODO'''\n",
    "        \n",
    "        variable_2 = '''TODO'''\n",
    "        \n",
    "        unigram_probabilities['''TODO'''] = '''TODO'''\n",
    "        \n",
    "    return unigram_probabilities\n",
    "\n",
    "\n",
    "def train():\n",
    "    '''\n",
    "    Implement the train function for our simplified method.\n",
    "    \n",
    "    Input: A list of preprocessed strings.\n",
    "    Output: A dictionary of counts of each string.\n",
    "    '''\n",
    "    unigram_counts_per_language = {}\n",
    "    \n",
    "    for language in languages:\n",
    "        \n",
    "        preprocessed_list_of_tokens = training_data[language]\n",
    "        \n",
    "        unigram_counts = count_tokens(preprocessed_list_of_tokens)\n",
    "        \n",
    "        unigram_counts_per_language[language] = unigram_counts\n",
    "        \n",
    "        \n",
    "    \n",
    "    vocabulary = set()\n",
    "\n",
    "    \n",
    "    for language in languages:\n",
    "        \n",
    "        language_tokens = list(unigram_counts_per_language[language].keys())\n",
    "        \n",
    "        \n",
    "        for token in language_tokens:\n",
    "            \n",
    "            vocabulary.add(token)\n",
    "            \n",
    "\n",
    "    unigram_probabilities_per_language = {}\n",
    "    \n",
    "    for language in languages:\n",
    "        \n",
    "        unigram_counts = unigram_counts_per_language[language]\n",
    "        \n",
    "        unigram_probabilities = compute_probabilities(unigram_counts, vocabulary)\n",
    "        \n",
    "        unigram_probabilities_per_language[language] = unigram_probabilities\n",
    "            \n",
    "\n",
    "    return unigram_probabilities_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Run the \"train\" function on the text of each language by running the cell below.\n",
    "\n",
    "`unigram_dictionaries` will contain a dictionary for each language with each token's probability of occurring in that language based on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_dictionaries = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Documentation - spend a minute documenting how the `train` procedure works in plain English (you can use the **Train function attempt 1** slide in the template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test Time\n",
    "\n",
    "We are given a dictionary of text called `test` that looks just like the `train` dictionary from earlier. We want to test how well our model can predict the languages by testing it on those strings--since we know the true answer (we refer to the true answer as the `ground_truth`), we can count the number of strings for which the model predicts the correct language. We then compute the **accuracy** of the model, which is the number correct divided by total number strings we tested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. `test_data`\n",
    "\n",
    "The first step is to preprocess the `test` data the same way as we did for the `train` data. We save the preprocessed test data in a list called `test_data`, and we save each sentence's actual language in a list called `ground_truth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "ground_truth = []\n",
    "\n",
    "for language in languages:\n",
    "    \n",
    "    text_list = test[language]\n",
    "    \n",
    "    for sentence in text_list:\n",
    "        preprocessed_list_of_tokens = preprocess(sentence)\n",
    "        test_data.append(preprocessed_list_of_tokens)\n",
    "        ground_truth.append(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. `ground_truth`\n",
    "\n",
    "Since we added the test sentences to `test_data` at the same time that we added their languages to `ground_truth`, then the index position in `ground_truth` gives the answer to the string at the same index position in `test_data`. Run the cell below to observe this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ground_truth[0], '-', test_data[0])\n",
    "print(ground_truth[12], '-', test_data[12])\n",
    "print(ground_truth[25], '-', test_data[25])\n",
    "print(ground_truth[40], '-', test_data[40])\n",
    "print(ground_truth[-13], '-', test_data[-13])\n",
    "print(ground_truth[-1], '-', test_data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Random baseline\n",
    "* Language detection is a **classification task.** In order to believe that a model is actually learns anything, we would want to see significant performance over a **random baseline,** which is a \"model\" in a sense. This \"model\" is one that \"guesses\" the language of each sample randomly.  \n",
    "\n",
    "* In this project, we have six languages, and our test set has the same number of sentences for each language, so we should expect our **random baseline** model to perform at about 16.67% accuracy. \n",
    "\n",
    "* Run the cell below to observe the count of each language's test strings and the random baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_count = Counter(ground_truth)\n",
    "\n",
    "print('Language','|', '# Test Items')\n",
    "for language, count in language_count.items():\n",
    "    print(language,'|', count)\n",
    "print('-' * 15, '\\n', 'Total','|', sum(language_count.values()))\n",
    "print(\"\\nAccuracy of randomly guessing: about {:.2%}\".format(Counter(ground_truth)['English']/len(ground_truth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Run `identify_language` on the test strings\n",
    "\n",
    "Running the cell below will:\n",
    "- go through each `test_sentence` and run the `identify_language` function on the `test_sentence`\n",
    "- save the language returned from `identify_language` in a list of `predictions`\n",
    "- show you the predictions for the first ten `test_sentences` saved in the `predictions` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_language(test_sentence, languages, unigram_dictionaries):\n",
    "    '''\n",
    "    (Solution) Fill in this function to predict the language of test_string\n",
    "    \n",
    "    input: test_string, list of our languages, unigram_dicts\n",
    "    output: one of the languages\n",
    "    '''\n",
    "    \n",
    "    language_probabilities = {language:1 for language in languages} # initialize probabilities to 1\n",
    "    \n",
    "    for language in languages: # compute the probability of the test sentence appearing in each language\n",
    "            \n",
    "        for token in test_sentence:\n",
    "\n",
    "            unigram = token\n",
    "\n",
    "            probability_of_token = unigram_dictionaries[language][unigram]\n",
    "            \n",
    "            language_probabilities[language] = language_probabilities[language] * probability_of_token\n",
    "\n",
    "    highest_probability_language = get_highest_probability_language(language_probabilities) # pre-written function to get the language with highest probability\n",
    "    \n",
    "    return highest_probability_language\n",
    "\n",
    "predictions = []\n",
    "for test_sentence in test_data:\n",
    "    prediction = identify_language(test_sentence, languages, unigram_dictionaries)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "print(\"Predictions for first 10 test sentences:\\n\", predictions[:10])\n",
    "print(\"Ground truth for first 10 test sentences:\\n\", ground_truth[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Run the cell to call the function `evaluate` \n",
    "\n",
    "`evaluate` will compute the accuracy of your model's predictions. Remember, we hope to beat at least the random baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, data = evaluate(predictions, ground_truth, test_data)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Documentation - write down the accuracy score of this model in the template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Run the cell below to conceptualize what `data` looks like.\n",
    "\n",
    "The evaluate function also created a data structure called `data`. For each ground truth language, `data` keeps track of which language each string was predicted as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data_structure(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Qualitatively analyze the predictions compared to ground truth \n",
    "\n",
    "- Using `data`, we can create a visual to help us perform error analysis.\n",
    "\n",
    "- Run the cell below to call the function `actual_vs_predicted`. It will output a figure called a *confusion matrix* that will show us the number of English samples correctly classified as English, incorrectly classified as German and Dutch, and the same for the other languages. \n",
    "\n",
    "- If our model is perfect, the square's diagonal would have `12` in every cell. \n",
    "\n",
    "- The function `actual_vs_predicted_text(data)` will output descriptive text for the matrix to help you interpret this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted(languages, data)\n",
    "actual_vs_predicted_text(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Documentation - paste the confusion matrix into your experiment template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analysis\n",
    "\n",
    "Did your model operate perfectly? Likely, there are at least a few errors. Use the space below to study the predictions made by your model. Try to answer the following questions with your analysis, and document in your template:\n",
    "\n",
    "### 1. Are there clear differences between the strings that the model predicted **English** correctly, and the strings the model incorrectly predicted to be **German** for English strings instead?\n",
    "### 2. Are there clear differences between the strings that the model predicted **English** correctly, and the strings the model incorrectly predicted to be **Dutch** for English strings instead?\n",
    "### 3. Are there clear differences between the strings that the model predicted **German** correctly, and the strings the model incorrectly predicted to be **English** for German strings instead?\n",
    "### 4. Are there clear differences between the strings that the model predicted **German** correctly, and the strings the model incorrectly predicted to be **Dutch** for German strings instead?\n",
    "### 5. Are there clear differences between the strings that the model predicted **Dutch** correctly, and the strings the model incorrectly predicted to be **English** for Dutch strings instead?\n",
    "### 6. Are there clear differences between the strings that the model predicted **Dutch** correctly, and the strings the model incorrectly predicted to be **German** for Dutch strings instead?\n",
    "### 7. If you were to modify your preprocessing steps, what do you think you could do to improve the model?\n",
    "### 8. Can you think of another possible model for language detection that might work better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "use the space below to code to help you answer these questions\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. If you beat the random baseline, you have established a *better* baseline! Can you beat your new baseline? If you did not beat the random baseline, try to get a better one!\n",
    "\n",
    "Go back and try to make some fixes to your model to beat your baseline!! Document your experiments in the template under the **Attempt 2 - beat the baseline** slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
