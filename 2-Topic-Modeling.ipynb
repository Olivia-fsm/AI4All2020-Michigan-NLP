{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are importing functions written by staff to help with the project. You will not need to worry about these, but if you are curious to see what they look like, checkout the file `tm_helpers.py`. If you want to learn more about how these functions or have questions, let us know! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tm_helpers import *\n",
    "\n",
    "# general\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import regex as re\n",
    "\n",
    "# preprocess functions\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "\n",
    "# topic modeling packages\n",
    "import gensim\n",
    "from gensim import models, corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# used to visualize the topic model\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In this section you will:\n",
    "\n",
    "1. Load bookcorpus\n",
    "2. Cleanup stopwords\n",
    "3. Join bigrams and trigrams\n",
    "4. Final data preparation for gensim topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's preview what the data looks like.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data length:\", len(data))\n",
    "print(\"data[:10]\", data[:10]) # data is a list, data[:10] is the first ten items of that list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_line(line):\n",
    "    '''\n",
    "    Fill in this function. Refer to 1-Intro-to-NLP for preprocessing ideas.\n",
    "    '''\n",
    "    preprocessed_line = []\n",
    "    tokens = word_tokenize(line)\n",
    "    \n",
    "    # use spacy pipeline\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    \n",
    "    # allowed_postags\n",
    "#     allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    allowed_postags=['NOUN']\n",
    "    \n",
    "    \n",
    "    # get pos_tags\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    \n",
    "    # get_lemmas, also remove words that aren't in allowed pos tags, also remove stopwords\n",
    "    lemmas = [token.lemma_ for token in doc if token.pos_ in allowed_postags and not token.is_stop]\n",
    "    \n",
    "    \n",
    "    preprocessed_line = lemmas\n",
    "    \n",
    "    return preprocessed_line\n",
    "\n",
    "def preprocess(data):\n",
    "    preprocessed_data = []\n",
    "    for line in tqdm(data):\n",
    "        preprocessed_line = preprocess_line(line)\n",
    "        preprocessed_data.append(preprocessed_line)\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A stopword solution\n",
    "'''\n",
    "\n",
    "# STOPWORDS = load_stopwords()\n",
    "# print(\"Cleaning data\")\n",
    "# tokenized_data = []\n",
    "# for text in tqdm(data):\n",
    "#     cleaned = clean_text(text, STOPWORDS)\n",
    "#     tokenized_data.append(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run call `preprocess` on `data` and save to `preprocessed_data`, and then preview our `preprocessed_data`. How does is look different than our earlier preview? Do you have a bug or does it look how you want it to look?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = preprocess(data)\n",
    "print(\"data[:10]\", preprocessed_data[:10]) # data is a list, data[:10] is the first ten items of that list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Join bigrams and trigrams\n",
    "**Next we will train a bigram model by using functions implemented by others (gensim) for us to use!**\n",
    "\n",
    "TODO: I think I will have a \"basic nlp\" preprocessing project, where they'll learn what bigrams and trigrams are. They would do this before the topic modeling lesson, so we wouldn't need to explain what bigrams and trigrams are.\n",
    "\n",
    "TODO: Add a link for reference to the gensim bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model, bigram_phrases = train_bigram_model(preprocessed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we apply our bigram model to our data to join unigrams into bigrams where appropriate. To understand what the changes look like, the `preview_bigram_changes` function will output a few examples of lines that were changed by this process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_bigrams = make_bigrams(preprocessed_data, bigram_model)\n",
    "\n",
    "show_ngrams(preprocessed_data, data_words_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we will train a trigram model!**\n",
    "\n",
    "TODO: Add a link for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model = train_trigram_model(preprocessed_data, bigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we apply our bigram model to our data to join unigrams into bigrams where appropriate. To understand what the changes look like, the `preview_bigram_changes` function will output a few examples of lines that were changed by this process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_trigrams = make_trigrams(data_words_bigrams, bigram_model, trigram_model)\n",
    "show_ngrams(data_words_bigrams, data_words_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final data preparation for gensim topic modeling\n",
    "\n",
    "Todo: for now, I'm just taking a chunk of the data to speed up my developing process. I can go through the data and choose a couple appropriate books that the students can actually use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating dictionary and corpus instances for gensim...\", end='')\n",
    "\n",
    "dictionary = corpora.Dictionary(data_words_trigrams[-10000:])\n",
    "corpus = [dictionary.doc2bow(x) for x in data_words_trigrams[-10000:]]\n",
    "\n",
    "print(\"complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Topic Model\n",
    "\n",
    "In this section, you will:\n",
    "\n",
    "1. Learn about topic model parameters\n",
    "2. Create a topic model\n",
    "3. Observe words associated with the topics\n",
    "4. Evaluate quantitatively\n",
    "5. Assign text lines to a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_EVERY = 10\n",
    "CHUNKSIZE = 100\n",
    "# CHUNKSIZE = 10 soln\n",
    "\n",
    "PASSES = 10\n",
    "topic_model_settings = [{'num-topics':15, 'parameters':{'random_state':100, 'update_every':UPDATE_EVERY, 'chunksize':CHUNKSIZE, 'passes':PASSES, 'alpha':'auto', 'per_word_topics':False}}, \n",
    "                        {'num-topics':5,'parameters':{'random_state':100, 'update_every':UPDATE_EVERY, 'chunksize':CHUNKSIZE, 'passes':PASSES, 'alpha':'auto', 'per_word_topics':False}}, \n",
    "                        {'num-topics':10,'parameters':{'random_state':100, 'update_every':UPDATE_EVERY, 'chunksize':CHUNKSIZE, 'passes':PASSES, 'alpha':'auto', 'per_word_topics':False}}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learn about topic model parameters\n",
    "\n",
    "Todo: add some things to show what the purpose of the main parameters are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We've already come up with a few different parameter settings, the main difference being the number of topics we are targetting. Let's start with the first setting, which will create 15 topics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = topic_model_settings[0]\n",
    "NUM_TOPICS = setting['num-topics']\n",
    "NUM_TOPICS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a topic model\n",
    "\n",
    "**Now we create a topic model for our text using functions from gensim! This will take a few minutes, so take this time to review some content. You can check out [this resource](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/) on topic modeling as well, which will show several of the steps we have already covered in a bit more detail, and give you a preview of what we will do next!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training topic model (this will take a moment)...\", end='')\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary, random_state = setting['parameters']['random_state'], update_every = setting['parameters']['update_every'], chunksize = setting['parameters']['chunksize'], passes = setting['parameters']['passes'], alpha = setting['parameters']['alpha'], per_word_topics = setting['parameters']['per_word_topics'])\n",
    "print(\"complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Observe words associated with the topic\n",
    "\n",
    "The columns of `topic_terms` are the top ten words of each topic (Wn) and Wn's probability of belonging to each topic (Wn Pr). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms = show_topic_terms(lda_model, NUM_TOPICS)\n",
    "topic_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate quantitatively\n",
    "\n",
    "**We will use two measures to evaluate the model: perplexity and coherence**\n",
    "\n",
    "Todo: I'll expand on this a little bit to explain what these metrics mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "6. Measure quality of topic models with perplexity\n",
    "'''\n",
    "print(\"Measuring model perplexity...\",end=\"\")\n",
    "ppl = lda_model.log_perplexity(corpus)\n",
    "print('complete. Perplexity:', ppl, '\\n')  # a measure of how good the model is. lower the better.\n",
    "\n",
    "'''\n",
    "7. Measure quality of topic models with coherence\n",
    "'''\n",
    "print(\"Measuring model coherence...\",end=\"\")\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts = data_words_trigrams[-10000:], corpus=corpus, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('complete. Coherence Score:', coherence_lda, '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Assign text lines to a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import operator\n",
    "'''\n",
    "9. Save topic assignments for unique texts in a datastructure\n",
    "'''\n",
    "print(\"Making document topic assignments...\")\n",
    "text2distro = {}\n",
    "for x in range(len(corpus)):\n",
    "    topicdistribution = lda_model[corpus[x]]     # a list of tuples, e.g., [(8, 0.14625458), (10, 0.79183161)]\n",
    "    topicarray = [0]*NUM_TOPICS\n",
    "\n",
    "    for (topicid,topicprc) in topicdistribution:\n",
    "        topicarray[topicid] = topicprc\n",
    "    try:\n",
    "        text2distro[' '.join(data_words_trigrams[-10000:][x])] = topicarray\n",
    "    except:\n",
    "        print('x:', x, \"len(data_words_trigrams[-10000:]):\", len(data_words_trigrams[-10000:]))\n",
    "\n",
    "top_topics = defaultdict(lambda:0)\n",
    "second_top_topics = defaultdict(lambda:0)\n",
    "text2scores = defaultdict(lambda:0)\n",
    "\n",
    "for text in text2distro:\n",
    "    if len(text) > 1:\n",
    "        distro = text2distro[text]\n",
    "        idx2score = {i:score for i, score in enumerate(distro)}\n",
    "        scores_sorted = sorted(idx2score.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        top_topic = scores_sorted[0][0]\n",
    "        top_topics[top_topic] += 1\n",
    "        second_top_topics[scores_sorted[1][0]] += 1\n",
    "        text2scores[text] = scores_sorted\n",
    "\n",
    "# print(\"Top topic distribution:\\n\", top_topics)\n",
    "# print(\"\\n\\nSecond top topic distribution:\\n\", second_top_topics)\n",
    "\n",
    "print(\"Topic #\", '\\t', \"% docs assigned\")\n",
    "total = sum(top_topics.values())\n",
    "for i in range(NUM_TOPICS):\n",
    "    print(i, '\\t', \"{:.2%}\".format(top_topics[i]/total))\n",
    "print('complete.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize the topics\n",
    "\n",
    "**It will take a few moments to load up the visual**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model = models.LdaModel.load(model_names[0])\n",
    "# Visualize the topics\n",
    "\n",
    "print(\"Generating visual...this will take a few moments.\")\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Topics\n",
    "\n",
    "We might not do this, but I have the code...it depends on if we want to do any analysis with this topic later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_threshold = .01 # put the percentage (.01 = 1%) threshold of documents assigned to the topic in order to be considered for naming\n",
    "KEPT_TOPICS = [i for i in range(NUM_TOPICS) if top_topics[i]/total >= keep_threshold]\n",
    "\n",
    "print(\"Kept Topics, which have {:.2%} of the docs assigned to them\".format(keep_threshold))\n",
    "print(KEPT_TOPICS)\n",
    "KEPT_TOPIC_NAMES = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_topics(KEPT_TOPICS, KEPT_TOPIC_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://nlpforhackers.io/topic-modeling/\n",
    "#https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/  -- for more analyses of topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
