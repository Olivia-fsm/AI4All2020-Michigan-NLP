{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Save2Drive](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/save2drive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "0. Instructions\n",
    "1. Intro to topic modeling reading (~5 minutes)\n",
    "2. Setup\n",
    "3. Preview\n",
    "4. Load data\n",
    "5. First preprocessing experiment: Implement a function for the first preprocessing experiment\n",
    "6. Create your first topic model\n",
    "7. Qualitative observations of first topic model.\n",
    "8. Quantitative observations of first topic model.\n",
    "9. Second preprocessing experiment: Implement a different preprocessing function for the second preprocessing experiment and run topic modeling pipeline on new preprocessed data.\n",
    "11. Parameter Experiment\n",
    "12. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1. This notebook will guide you through each step of the project. \n",
    "2. Throughout the project, you will record your experiments and observations. We have created a slide deck template for you that you can download and edit in Google Slides or Microsoft Power Point. The template is at **`Experiment-Report-Templates/2-Topic-Modeling-Template.pptx`**.\n",
    "3. We are happy to help with *anything*. Ask away :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Topic Modeling Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TM](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/tm-0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TM](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/tm-1.png)\n",
    "![TM](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/tm-2.png)\n",
    "![TM](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/tm-3.png)\n",
    "![TM](https://raw.githubusercontent.com/alahnala/AI4All2020-Michigan-NLP/master/slides/tm-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the cell below below to get setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  !rm -r AI4All2020-Michigan-NLP\n",
    "  !git clone https://github.com/alahnala/AI4All2020-Michigan-NLP.git\n",
    "  !cp -r AI4All2020-Michigan-NLP/utils/ .\n",
    "  !cp -r AI4All2020-Michigan-NLP/Data/ .\n",
    "  !cp -r AI4All2020-Michigan-NLP/slides/ .\n",
    "  !cp -r AI4All2020-Michigan-NLP/Experiment-Report-Templates/ .\n",
    "  !echo \"=== Files Copied ===\"\n",
    "\n",
    "import pandas as pd\n",
    "# used to visualize the topic model\n",
    "%pip install --upgrade gensim\n",
    "%pip install pyldavis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from utils.nlp_basics import *\n",
    "from utils.syllable import *\n",
    "\n",
    "import utils.tm_helpers as helpers\n",
    "\n",
    "# general\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import regex as re\n",
    "\n",
    "# preprocess functions\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "\n",
    "# topic modeling packages\n",
    "import gensim\n",
    "from gensim import models, corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview\n",
    "\n",
    "By the end of this project, you will have experimented with preprocessing steps, and generate an interactive visual to explore the topics. Run the cell below to get a preview of the end goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.show_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Run the cell below to load the data that we will use for topic modeling. The data is saved into a *list* data structure called `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = helpers.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get an idea of what `data` looks like, run the cell below to see what the first  looks like.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data length:\", len(data), \"First 10 items in data:\\n\")\n",
    "\n",
    "# data is a list, data[:10] is the first ten items of that list\n",
    "for i, item in enumerate(data[:10]):\n",
    "    print(\"{}:\".format(i), item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Experiment 1\n",
    "\n",
    "## Write the function `preprocess_line`.\n",
    "1. At this point, *preprocessing* is the main piece to experiment with and observe your implementation decisions on topic modeling. Using at least one new thing that you learned in 1-Intro-to-NLP, write the function `preprocess_line` which takes in a string of text saved in the input variable `line`, so that the function returns a list of tokens called `preprocessed_line`. \n",
    "2. At the bottom of the cell, we call `preprocess_line` on `test_string` so you can quickly observe how your function is working. `test_string` is initially set to be the first string from `data`, but you can change `test_string` to another string from `data` or your own string to get a fuller idea of the effects of your function.\n",
    "3. Take a second to record your decisions in the slide titled **Preprocessing Experiment 1**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_line(line):\n",
    "    '''\n",
    "    Fill in this function. Refer to 1-Intro-to-NLP for preprocessing ideas.\n",
    "    '''\n",
    "    preprocessed_line = line.split()\n",
    "\n",
    "    \n",
    "    return preprocessed_line\n",
    "\n",
    "test_string = data[0]\n",
    "print(preprocess_line(test_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run `preprocess` on all of the data.\n",
    "1. Run the cell below to `preprocess` all of the strings in `data` and save to `preprocessed_data` \n",
    "2. The cell will output a preview our `preprocessed_data`. How does is look different than our earlier preview? Do you have a bug or does it look how you want it to look? \n",
    "3. Take a second to record your observations in the slide deck, in the slide titled **Preprocessing Output Observations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    preprocessed_data = []\n",
    "    for line in tqdm(data):\n",
    "        preprocessed_line = preprocess_line(line)\n",
    "        preprocessed_data.append(preprocessed_line)\n",
    "    return preprocessed_data\n",
    "preprocessed_data = preprocess(data)\n",
    "\n",
    "# data is a list, data[:10] is the first ten items of that list\n",
    "for i, item in enumerate(preprocessed_data[:10]):\n",
    "    print(\"{}:\".format(i), item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final data preparation for gensim topic modeling\n",
    "\n",
    "1. In this step, we use functions from the `gensim` module to create `dictionary` and `corpus` in the format that the topic modeling function requires.\n",
    "2. `dictionary`: This is our *set of words* after our preprocessing, which we will use for creating different probability distributions. This *set of words* is often referred to as the *vocabulary* in NLP terminology.\n",
    "3. `corpus`: Words are typically converted into a numerical representation before using them in a model. To demonstrate this, the cell will print each word for the first item in your preprocessed data next to its numerical encoding. Why is the word **information** represented as **(3, 1)**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating dictionary and corpus instances for gensim...\", end='')\n",
    "\n",
    "dictionary = corpora.Dictionary(preprocessed_data)\n",
    "corpus = [dictionary.doc2bow(x) for x in preprocessed_data]\n",
    "\n",
    "print(\"complete.\\n\")\n",
    "\n",
    "print(dictionary, '\\n')\n",
    "\n",
    "\n",
    "for original, encoded in zip(preprocessed_data[0], corpus[0]):\n",
    "    print(\"Original: {}\".format(original), \"--->  Encoded: {}\".format(encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Topic Model\n",
    "\n",
    "\n",
    "1. Run the cell below to create a topic model for our data, and the topic model will be saved in the variable `lda_model`. For now, do not worry about the parameters, we will experiment with those later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following lines are used to setup the model's parameters. \n",
    "'''\n",
    "UPDATE_EVERY = 10\n",
    "CHUNKSIZE = 10\n",
    "PASSES = 10\n",
    "topic_model_settings = [{'num-topics':15, 'parameters':{'random_state':100, 'update_every':UPDATE_EVERY, 'chunksize':CHUNKSIZE, 'passes':PASSES, 'alpha':'auto', 'per_word_topics':False}}, \n",
    "                        {'num-topics':5,'parameters':{'random_state':100, 'update_every':UPDATE_EVERY, 'chunksize':CHUNKSIZE, 'passes':PASSES, 'alpha':'auto', 'per_word_topics':False}}, \n",
    "                        {'num-topics':10,'parameters':{'random_state':100, 'update_every':UPDATE_EVERY, 'chunksize':CHUNKSIZE, 'passes':PASSES, 'alpha':'auto', 'per_word_topics':False}}]\n",
    "setting = topic_model_settings[0]\n",
    "NUM_TOPICS = setting['num-topics']\n",
    "\n",
    "\n",
    "'''\n",
    "Here, we plug in our parameters and data into models.LdaModel to train the topic model. The topic model will be saved in the variable \"lda_model\".\n",
    "'''\n",
    "print(\"Training topic model of {} topics (this might take a moment)...\".format(NUM_TOPICS), end='')\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary, random_state = setting['parameters']['random_state'], update_every = setting['parameters']['update_every'], chunksize = setting['parameters']['chunksize'], passes = setting['parameters']['passes'], alpha = setting['parameters']['alpha'], per_word_topics = setting['parameters']['per_word_topics'])\n",
    "print(\"complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Observations\n",
    "\n",
    "We have created a topic model and saved it in `lda_model`. But what does this mean? Let's learn more as we perform some qualitative observations. Later, we will discuss two quantitative metrics, but a lot of times, topic models are decided from manual observation.\n",
    "\n",
    "## Topic Terms\n",
    "1. Run the cell below to observe the topic terms.  \n",
    "2. By creating the topic model, we have created a probability distribution over our set of words (`dictionary` or the vocabulary) for 15 topics. We have created a helper function called `show_topic_terms` that will extract the ten words with the highest probabilities for fitting into each topic. \n",
    "3. The columns of `topic_terms` are the top ten words of each topic (Wn) and Wn's probability of belonging to each topic (Wn Pr). Each row is a topic.\n",
    "4. What do you think of these topics? Can you make sense of clear topic divisions between the 15 topics, or, would you be able to give a name to each topic? How could you modify the `preprocess_line` function to possibly improve the topic model to get a clearer division between topics?\n",
    "5. Before you make any changes or proceed, make notes of your observations in the slide **Topic Model Qualitative Observations**. You can include a screenshot of part of this table, paste the table values into the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms = helpers.show_topic_terms(lda_model, NUM_TOPICS)\n",
    "topic_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model Visualization\n",
    "\n",
    "1. Run the cell below to generate an interactive visual to explore the topic model.\n",
    "2. Add observations to the **Topic Model Qualitative Observations** slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.show_model(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Observations\n",
    "\n",
    "1. We will compute *perplexity* and *coherence,* which are two quantitative metrics for evaluating topic models. Better scores do not necessarily mean better models in topic modeling. \n",
    "2. Add the scores to the **Topic Model 1 Quantitative Observations** slide.\n",
    "\n",
    "\n",
    "## Perplexity\n",
    "\n",
    "\"In information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample ([Wikipedia](https://en.wikipedia.org/wiki/Perplexity)).\"\n",
    "\n",
    "Intuition: the dictionary definition of perplexed is \"Completely baffled; very puzzled [Dictionary Reference](https://www.lexico.com/en/definition/perplexed).\" The more perplexed you are, the more puzzled you are. Use this to remember the intuition behind the *perplexity* metric. We aim for a model that less perplexed by new data, so a smaller perplexity is usually the goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Measure quality of topic models with perplexity\n",
    "'''\n",
    "print(\"Measuring model perplexity...\",end=\"\")\n",
    "ppl = lda_model.log_perplexity(corpus)\n",
    "print('complete. Perplexity:', ppl, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence\n",
    "\n",
    "The coherence metric captures the \"degree of semantic similarity between high scoring words in the topic ([Towards Data Science](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)).\"\n",
    "\n",
    "The idea behind the *coherence* metric aligns with your experience observing your topic terms. Did you observe a very clear theme in each topic or was there a lot of overlapping themes between the topics? Clear themes are analogous to higher coherence scores, whereas overlapping themes are analogous to distinct topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Measure quality of topic models with coherence\n",
    "'''\n",
    "print(\"Measuring model coherence...\",end=\"\")\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts = preprocessed_data, corpus=corpus, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('complete. Coherence Score:', coherence_lda, '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Experiment 2\n",
    "\n",
    "## Write the function `preprocess_line_2`.\n",
    "1. As you did with `preprocessed_line`, implement the function `preprocessed_line_2`. Try to base your new decisions on your previous observations.  \n",
    "2. Test your function on a `test_string`.\n",
    "3. Take a second to record your decisions in the slide titled **Preprocessing Experiment 2**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_line_2(line):\n",
    "    '''\n",
    "    Fill in this function. Refer to 1-Intro-to-NLP for preprocessing ideas.\n",
    "    '''\n",
    "    preprocessed_line = line.split()\n",
    "    \n",
    "\n",
    "    \n",
    "    return preprocessed_line\n",
    "\n",
    "test_string = data[0]\n",
    "print(preprocess_line_2(test_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Topic Modeling pipeline for preprocessing experiment 2\n",
    "\n",
    "1. Run the following cell to create a new topic model and output the quantitative metrics. Add your observations to the **Topic Model 2 Quantitative Observations** slide.\n",
    "2. The next cell will output the topic terms. Add your observations to the **Topic Model 2 Qualitative Observations** slide.\n",
    "3. The third cell will output the interactive visual. Add your observations to the **Topic Model 2 Qualitative Observations** slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Preprocess all data\n",
    "'''\n",
    "def preprocess_2(data):\n",
    "    preprocessed_data = []\n",
    "    for line in tqdm(data):\n",
    "        preprocessed_line = preprocess_line_2(line)\n",
    "        preprocessed_data.append(preprocessed_line)\n",
    "    return preprocessed_data\n",
    "preprocessed_data = preprocess_2(data)\n",
    "\n",
    "for i, item in enumerate(preprocessed_data[:10]):\n",
    "    print(\"{}:\".format(i), item)\n",
    "    \n",
    "    \n",
    "'''\n",
    "2. Create corpus and dictionary\n",
    "'''   \n",
    "print(\"Creating dictionary and corpus instances for gensim...\", end='')\n",
    "dictionary = corpora.Dictionary(preprocessed_data)\n",
    "corpus = [dictionary.doc2bow(x) for x in preprocessed_data]\n",
    "print(\"complete.\\n\")\n",
    "\n",
    "'''\n",
    "3. Create topic model\n",
    "'''\n",
    "print(\"Training topic model of {} topics (this might take a moment)...\".format(NUM_TOPICS), end='')\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary, random_state = setting['parameters']['random_state'], update_every = setting['parameters']['update_every'], chunksize = setting['parameters']['chunksize'], passes = setting['parameters']['passes'], alpha = setting['parameters']['alpha'], per_word_topics = setting['parameters']['per_word_topics'])\n",
    "print(\"complete.\")\n",
    "\n",
    "'''\n",
    "4. Measure quality of topic model with perplexity\n",
    "'''\n",
    "print(\"Measuring model perplexity...\",end=\"\")\n",
    "ppl = lda_model.log_perplexity(corpus)\n",
    "print('complete. Perplexity:', ppl, '\\n')\n",
    "\n",
    "'''\n",
    "5. Measure quality of topic models with coherence\n",
    "'''\n",
    "print(\"Measuring model coherence...\",end=\"\")\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts = preprocessed_data, corpus=corpus, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('complete. Coherence Score:', coherence_lda, '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "6. Print topic terms\n",
    "'''\n",
    "topic_terms = helpers.show_topic_terms(lda_model, NUM_TOPICS)\n",
    "topic_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "7. Generate interactive visual\n",
    "'''\n",
    "helpers.show_model(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Experiment\n",
    "\n",
    "1. Another impact on the quality of the topics are the different parameters.\n",
    "2. For some, it is fun to learn by experience and just trying different values and making observations about the effect. Others may prefer to look at documentation or [tutorials](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/) for guidance. Reading documentation for code and libraries can be another skill to work on. Try taking a look at the [Gensim LDA Documentation](https://radimrehurek.com/gensim/models/ldamodel.html) and/or [a tutorial](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/) to see what you can learn about the different parameters.\n",
    "3. Try experimenting with changing at least one parameter in the cell below, and report what parameters you are experimenting with in the **Topic Model Parameter Experiment** slide. (Hint: a good start is NUM_TOPICS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_EVERY = 10\n",
    "CHUNKSIZE = 10\n",
    "PASSES = 10\n",
    "topic_model_settings = [{'num-topics':15, 'parameters':{'random_state':100, 'update_every':UPDATE_EVERY, 'chunksize':CHUNKSIZE, 'passes':PASSES, 'alpha':'auto', 'per_word_topics':False}}, \n",
    "                        {'num-topics':5,'parameters':{'random_state':100, 'update_every':UPDATE_EVERY, 'chunksize':CHUNKSIZE, 'passes':PASSES, 'alpha':'auto', 'per_word_topics':False}}, \n",
    "                        {'num-topics':10,'parameters':{'random_state':100, 'update_every':UPDATE_EVERY, 'chunksize':CHUNKSIZE, 'passes':PASSES, 'alpha':'auto', 'per_word_topics':False}}]\n",
    "setting = topic_model_settings[0]\n",
    "NUM_TOPICS = setting['num-topics']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Topic Model with New Parameters\n",
    "\n",
    "Run the cell to create your new topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training topic model (this will take a moment)...\", end='')\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary, random_state = setting['parameters']['random_state'], update_every = setting['parameters']['update_every'], chunksize = setting['parameters']['chunksize'], passes = setting['parameters']['passes'], alpha = setting['parameters']['alpha'], per_word_topics = setting['parameters']['per_word_topics'])\n",
    "print(\"complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Metrics\n",
    "\n",
    "Report the scores in the **Parameter Experiment Quantitative Observations** slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Measure quality of topic model with perplexity\n",
    "'''\n",
    "print(\"Measuring model perplexity...\",end=\"\")\n",
    "ppl = lda_model.log_perplexity(corpus)\n",
    "print('complete. Perplexity:', ppl, '\\n')\n",
    "\n",
    "'''\n",
    "Measure quality of topic models with coherence\n",
    "'''\n",
    "print(\"Measuring model coherence...\",end=\"\")\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts = preprocessed_data, corpus=corpus, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('complete. Coherence Score:', coherence_lda, '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Observations\n",
    "\n",
    "The columns of `topic_terms` are the top ten words of each topic (Wn) and Wn's probability of belonging to each topic (Wn Pr). \n",
    "\n",
    "Report your observations in the **Parameter Experiment Qualitative Observations** slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms = helpers.show_topic_terms(lda_model, NUM_TOPICS)\n",
    "topic_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.show_model(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "By now, you have experimented with two preprocessing functions and saw their impact on topic modeling. Spend a few minutes reflecting on what you learned from your experience and put a few notes in the **Topic Modeling Conclusions** slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://dl.acm.org/doi/fullHtml/10.1145/2133806.2133826\n",
    "2. https://nlpforhackers.io/topic-modeling/\n",
    "3. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/  -- for more analyses of topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
